\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage{endnotes}
\usepackage{tikz}
\usepackage{adjustbox}
\title{From Macrostructure to Micromotives: From Macrostructure to Micromotives: Regression for discrete outomes with spatial autocorrelation}

\author{Forest Gregg}

\usetikzlibrary{arrows}
\usetikzlibrary{bayesnet}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\let\footnote=\endnote
\let\cite=\citep

\begin{document}
\maketitle
\begin{abstract}
  Racial segregation is one of many important social phenomena where a
  discrete outcome at one site depends upon the outcomes of
  neighboring sites. While we have methods to estimate systems where
  there is spatial auto-correlation when outcomes are continuous or
  counts, we have not had a method to estimate parameters of large
  systems of coupled, discrete outcomes in such spatial
  data. Sociologists have explored such systems through agent-based
  modeling, and in the case of racial segregation, have uncovered
  surprising results, such as the fact that racial preferences for
  neighbors could lead to large-scale racial segregation. However, we
  have not had a tools to go from observed global structures to
  estimates of local interactions. In this paper, we introduce a
  strategy for estimating local interactions from large systems of
  coupled, discrete variables: the structured support vector
  machine. Then we demonstrate this approach using detailed
  neighborhood data from the 30 largest U.S. cities to infer local
  housing choices, and outline a range of other social science
  applications
\end{abstract}


<<include=FALSE>>=
library(knitr)
library(RPostgreSQL)
library(xtable)
library(RcppCNPy)
opts_chunk$set(
echo=F, fig.width=4.5, fig.height=4.75, fig.path='/home/fgregg/sweave-cache/figs/fig', cache=T, results='hide'
)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
@ 

<<blockgroups-intercept, results='asis'>>=
tract_params <- read.csv('params_blockgroups.csv', header=FALSE)
names(tract_params) <- c('City',
                         'Int., Black', 'Median Housing Cost, Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 'Median Housing Cost, Hispanic', 
                         'H-B', 'H-W')

no_x <- tract_params[, c('City',
                         'Int., Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 
                         'H-B', 'H-W')]
xtable(no_x[order(no_x$"B-W"),],
       caption="Block Groups, No Housing Costs")
@


<<tracts-intercept, results='asis'>>=
tract_params <- read.csv('params_tracts.csv', header=FALSE)
names(tract_params) <- c('City',
                         'Int., Black', 'Median Housing Cost, Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 'Median Housing Cost, Hispanic', 
                         'H-B', 'H-W')

no_x <- tract_params[tract_params['Median Housing Cost, Black'] == 0, 
                     c('City',
                       'Int., Black', 
                       'B-H', 'B-W',
                       'Int., Hispanic', 
                       'H-B', 'H-W')]
xtable(no_x[order(no_x$"B-W"),],
       caption="Tracts, No Housing Costs")
@

<<tracts-x, results='asis'>>=
tract_params <- read.csv('params_tracts.csv', header=FALSE)
names(tract_params) <- c('City',
                         'Int., Black', 'Median Housing Cost, Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 'Median Housing Cost, Hispanic', 
                         'H-B', 'H-W')

with_x <- tract_params[tract_params['Median Housing Cost, Black'] != 0, 
                     c('City',
                       'Int., Black', 
                       'B-H', 'B-W',
                       'Int., Hispanic', 
                       'H-B', 'H-W')]
xtable(with_x[order(with_x$"B-W"),],
       caption = 'Tracts, Controlling for Median Housing Costs')
@ 

Sociologists are interested in a number of social phenomena that have
discrete outcomes and where the outcomes of of adjacent
units are irreducibly interdependent--racial segregation and housing
choice; peer effects on the taking up of risky behavior within a high
school friend group; the relation between a social network and taste
preferences.

There are many, different difficulties in understanding these
phenomena, but they all share a common hurdle. We have not had an
effective method to estimate large system with both discrete outcomes
and interdependence between adjacent units. While we now have methods
to estimate spatially auto-correlated systems with continuous or count
outcomes, autocorrelated discrete outcomes are intractable to our the
classic methods.

The recent development of a method called the Structured Support
Vector Machine provides a means address these systems. While this
method does not provide all the familiar benefits of our common
statistical tools, we are now able to provide point estimates and
confidence intervals for wide set of previously inaccessible questions.

In this paper, we introduce this method using as an example the
classic problem of how city-wide racial segregation can emerge from
individual households with slight racists preferences for neighbors.
Before introducing the structured support vector machine, we should
examine why our classic approaches will not work.

\subsection*{Modeling local interaction}
Briefly, there are two broad types of explanations for racial
segregation in American cities. First, different areas have different
benefits and costs to the people who live there, like average rent,
commute times, or access to lovely parks. If different racial groups
differ in either their ability to pay for housing or in their tastes,
they will tend to sort into different areas. Second, if individuals
prefer to live near neighbors of the same race or, equivalently, don't
want to live near neighbors of different races, then large scale
segregation can emerge, even if there's no difference, beforehand,
between places or between the wealth or tastes of the racial groups.

In order to compare the power of these two explanations, we might want
to see which factor explained more of observed residential racial
segregation in American cities. For the moment, assume that we live in
a black and white world.

If we already knew that neighbors do not matter, we could use logistic
regression to model how the characteristics of each site effect the
race of the person at that site.  Let
$y_i$ indicate the race of a person living a the $i$th site and have
$y_i$ take a value of $1$ if the person is black and $0$ if
white. Each site $i$ has a set of $D$ observed attributes $(x_0, x_1,
... x_{D-1}, x_D)$. The parameters $(\beta_0, \beta_1,
... \beta_{D-1}, \beta_D)$ determine the influence of the attributes
of the $i$th site on race of the person living at that site.

\begin{align}
  \log\frac{\Pr(y_i = 1)}{\Pr(y_i = 0)} = \beta_0 + \beta_1x_{0i} + ... + \beta_dx_d
\end{align}

In order to bring in the effect of neighbors, we can have a model like 

\begin{align}
  \log\frac{\Pr(y_i = 1 \mid \{y_j : j \neq i\})}{\Pr(y_i = 0 \mid \{y_j : j \neq i\})} = \beta_0 + \beta_1x_{0i} + ... + \beta_dx_d + \rho\sum_{j \in \operatorname{N}(i)}y_j
\end{align}
The parameter $\rho$ is the effect of the
race of neighbors of site $i$ on the race of the person at site
$i$. $\operatorname{N}(i)$ is a function that returns the indices of
the neighbors of the $i$th site.

If we could estimate this second equation, we could compare the two main
proposed mechanisms for racial segregation. If $\rho$ is a
persistent and consequential component of the model, that would be
strong evidence for racial segregation as a continued, emergent result
of individual racism.

\section*{Estimation}
Our classic methods, whether frequentist or Bayesian, for estimating
parameters depend upon being able to calculate the likelihood of the
data. However, while the spatial model looks similar to the logistic
regression models, the full data likelihood turns out to be
intractable. The likelihood depends not just on the observed patterns
of racial segregation, but every single possible pattern of racial
segregation.

Let $\mathbf{z}$ be a possible pattern of racial segregation
and $\zeta$ be the set of all possible patterns.

\begin{align}
  Z = \sum_{\mathbf{z} \in \zeta} \exp\Big({\sum_i^N(\beta_0z_i + ... + \beta_dx_dz_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}z_iz_j}\Big)
\end{align}
and the full data log likelihood of an observed pattern of racial
segregation, $\mathbf{y}$, is 

\begin{align}
  \log\Pr(\mathbf{\beta}, \rho \mid \mathbf{y}) = (\sum_i^M(\beta_0y_i  +
  ... + \beta_dx_dy_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j)- Z
\end{align}

For two race and $N$ sites there are $2^N$ possible patterns in
$\zeta$. So, with just 30 sites, the calculating the require takes
over a billion arithmetic operations. For all but the smallest data
sets, calculating the likelihood is computationally intractable.

If we cannot calculate the likelihood, we cannot maximize the
likelihood to estimate parameters. There have been two main responses
to this computational barrier, pseudolikelihood, MCMC approxmiation,
and simulation.

\subsection*{Psuedolikelihood}
Originally proposed by Besag, the pseudolikelihood is

\begin{align}
  p(\boldsymbol{\theta} \mid \mathbf{y}) = \prod_i^N\Pr(y_i \mid \{y_j : j \neq i\})
\end{align}

This is effectively the 
so for our model, the log pseudolikelihood is

\begin{align}
  \log p(\boldsymbol{\beta}, \rho \mid \mathbf{y}) = &\sum_i^M(\beta_0y_i +
  ... + \beta_dx_{di}y_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j\\&- \sum_i^M\log(1 + e^{\beta_0 +
  ... + \beta_dx_{id} + \rho\sum_{j \in \operatorname{N}(i)}y_j})
\end{align}

Like the maximum likelihood estimate, the maximum pseudolikelihood
estimate is a consistent estimator. That is, as the, the number of
sites $M$ goes to infinity the maximum pseudolikelihood parameter
estimates $\hat{\boldsymbol{\theta}}$ converges to the true parameter
$\boldsymbol{\theta}$. This convergence is slower than the maximum
likelihood estimator, and the loss of efficiency increases with the
strength of the interdependence between sites\cite{Cresside 488}. The
bias of the estimator has not been fully characterized \cite{erg}.

Social scientists have used psuedolikelihood methods to study a
problem that looks like the near opposite of the problem of racial
segregation. Given an observed social network of nodes and edges, can
we estimate the parameters that determine the likelihood that two
nodes will share an edge. In the problem of racial segregation, we
want to know how the races of the neighbors of site $i$ will affect
the race of the household at site $i$. In this social network problem,
the task is to estimate how the attributes of nodes $j$ and $k$ will
make it more likely that they will be ``neighbors'', i.e. share a tie.

Ultimately, the problem can take the same mathematical form if the
likelihood that node $j$ and $k$ are connected depends upon the number
of other ties that both $j$ and $k$ have. Let $y_{ij}$ take a value of
$1$ if there is a tie between node $i$ and node $j$ and $0$ otherwise,
then we could express this interdependency as follows

\begin{align}
  \log\frac{\Pr(y_{ij} = 1 \mid \{y_{kl} : k \neq i\ \text{and } l \neq
    j\})}{\Pr(y_{ij} = 0 \mid \{y_{kl} : k \neq i\ \text{and } l \neq j\})} =
  \beta_0 + \beta_1x_{0ij} + ... + \beta_dx_{dij} + \rho(\sum_{l \neq
    j}y_{il} + \sum_{k \neq i}y_{kj})
\end{align}

However, even with pseudolikelihood
approach, the 






is close to the inverse of the problem of racial
segregation: given a set of nodes with attributes, what is the
probability that there will be an edge between those nodes.
networks\cite{strauss,ikeda,frank,wasserman pattison,wasserman
  robbins,van Dujin}

\subsection*{MCMC Estimate of the Maximum Likelihood Estimate}
Using Markov Chain Monte Carlo methods, it is possible to approximate
the likelihood and estimate the maximum likelihood estimate of
$\boldsymbol{\theta}$. While there has been good progress in refining
  MCMC methods for estimating these types of models \cite{geyer, moller,
    hughes}, the immense number of possible patterns make MCMC methods
  intractable for data with more than a thousand sites.

As Hughe and his coauthors point out, while MCMC methods become
intractable as the number of sites increase, a large number of sites
should increase our confidence in the pseudolikelihood estimator. In a
simulation study, Hughes et.al found that accurate estimation was not
possible with less than 900 site, regardless of the method, but that
at above this size that the pseudolikelihood estimator performed as
well as the MCMC maximum likelihood estimator.

\subsection*{Simulation}
Social scientists have largely 

Social scientists believe that many large scale social patterns 
emerge from individuals making local adjustments to one
another. Historically, these types of emergence theories have been,
historically, hard to test quantitatively. The irreducible
interdependence of outcomes presents technical difficulties that made
estimation of these types of models practically impossible. With
increased computing power and advances in statistical method, we now
have techniques to model coupled outcomes when the outcomes are
continuous or counts. However, we still have not had a means to
estimate models of coupled outcomes where the outcomes are discrete.

Fortunately, this has not prevented us from exploring these types of
models altogether. While we have not been able to estimate local
effects from observed global patterns, we have been able to explore
how local effects can shape global patterns, using agent-based
modeling.

Within sociology, the most famous example is the emergence model of
racial segregation, which explains stark, city wide patterns of racial
segregation as the result of individual households acting on mild
preferences for same-race neighbors. In his 1971 paper, Thomas
Schelling introduced a model with black and white agents, endowed them
with preferences, set rules for when and how each agent could move,
and ran the simulation forward.\cite{schelling_dynamic_1971} When he
set the preferences of the agents to even mildly prefer to be
surrounded by agents of the same color, the black and white agents
would eventually segregate into distinct neighborhoods. These large
scale patterns looked quite similar to patterns of actual, racial
segregation in contemporary American cities.

Since his pioneering work, researchers have improved substantially on
his method. In Elizabeth Bruch's exemplary, recent paper on how within
group income inequality and between inequality can effect racial
segregation, the preferences of the agents are informed by research
into individual racial preferences, the agents move over a realistic
geographic space and not an abstract grid, and she uses the simulation
to derive specific, suprising associations that should appear in
aggregate data and which she validates
empirically.\cite{bruch_how_2014}

Agent-based modeling lets researchers explore how local actions could
lead to global patterns. Ideally, we would also be able to go in the
opposite direction. We would derive local effects from global patterns
and test whether those local effects are consistent with theory and
other evidence. While the typical model sociologists would use to
estimate the local effects are not tractable, in this paper we
introduce an estimation technique that can be used---the structured support
vector machine.


The full data likelihood for this model, has normalizing constant that
depends upon every possible way that blacks and whites could live on
the i sites.

In, the likelihood equation for this model of this model (Equation
\ref{equation:likelihood}), there is a sum that depends on every
possible way that blacks and whites could live at all possible sites
.\cite{cressie_statistics_1993} If there are $M$ locations, there are
$2^M$ possible residential patterns for the two races. For all but the
very smallest number of sites, this sum cannot be practically
computed.
\begin{align}
\log(L(\mathbf{y} | b_0,\rho)) = b_0\sum_iy_i +
\frac{\rho}{2}\sum_{i,j}^\mathcal{N}y_iy_j -
\sum_{z_i,...,z_D}e^{b_0\sum_ky_k + \frac{\rho}{2}\sum_{k,l}^\mathcal{N}y_ky_l}
\label{equation:likelihood}
\end{align}
Where $\mathcal{N}$ is the set of pairs of indices of neighboring
blocks and $z_i,...,z_D$ denotes every single possible way that blacks
and whites could live on the $M$ sites.  

This enormous sum makes maximum likelihood estimation or Bayesian
techniques impossible for models with even a moderate number of
locations. The structured support vector machine lets us solve a very
similar problem, while avoiding that intractable sum.

\input{theory_and_math.tex}

\section*{Estimating Racial Preferences}
We now estimate the racial frustration costs from the observed
residential patterns of the 49 most populous American counties (Table~\ref{tab:counties}) .

<<countyTable, results='asis'>>=
library(RPostgreSQL)
con <- DBI::dbConnect(RPostgreSQL::PostgreSQL(), dbname="segregation")

counties <- DBI::dbGetQuery(con, 
    "SELECT name[3] || ', ' || name[4] AS place, 
            SUM(total)/1000000::FLOAT AS population,
            SUM(white)/1000000::FLOAT AS white,
            SUM(black)/1000000::FLOAT  AS black,
            SUM(hispanic)/1000000::FLOAT AS hispanic
     FROM (SELECT REGEXP_SPLIT_TO_ARRAY(\"NAME\", ', ') AS name, 
                  \"B03002_001E\" AS total,
	          \"B03002_003E\" as white, 
	          \"B03002_004E\" as black, 
                  \"B03002_012E\" as hispanic
           FROM race) AS t 
     GROUP BY name[3] || ', ' || name[4] ORDER BY SUM(total) DESC")

names(counties) <- c("County", "Total Population", "White", "Black", "Hispanic")


n_blockgroups <- DBI::dbGetQuery(con, "SELECT COUNT(*) FROM race")
    
print(xtable(counties,
             caption="Populations of 49 Largest Counties (In Millions)",
             label="tab:counties"),
      digits=2,
      include.rownames=FALSE)
@ 

From the 2010-2015 ACS 5-Year Estimates, we pull the population of
Hispanics, Non-Hispanic Whites, and Non-Hispanic African Americans for
every block group in all the counties.\cite{bureau_american_????} For every
block group, we label the block group as Hispanic, White, or Black
based on which group is most common in the block group.  Using the
associated 2014 TIGER/Line Shapefiles, we calculate the adjacency
matrix for the
blockgroups\cite{tiger/line_????,rey_pysal:_2007}. There are \Sexpr{n_blockgroups} block groups in the data.

We'll use these data to estimate the parameters of the following score
function. 

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + \sum_i^N\epsilon_i(y_i)
\end{align}

Where 

\begin{align}
  \epsilon_{i,j} = &w_0\operatorname{WW}(x_{i}, x_{j}) +
  w_1\operatorname{WB}(x_{i}, x_{j}) + 
  w_2\operatorname{WH}(x_{i}, x_{j}) \\
  &+ w_3\operatorname{BB}(x_i, x_j) +
  w_4\operatorname{BH}(x_i, x_j) +
  w_5\operatorname{HH}(x_i, x_j)
\end{align} 

and $\operatorname{WW}$ indicates that block $i$ and $j$ are both
labeled as ``White''; $\operatorname{WB}$ indicates a ``White'' block
and ``Black'' block; $\operatorname{WH}$ indicates indicates a
``White'' block and ``Hispanic'' block; and so on.

In addition, 

\begin{align}
  \epsilon_{i} = w_6 + & w_7\operatorname{W_1}(y_{i}) +  
          w_{10}\operatorname{H_1}(y_{i}) + w_{11} +
          w_{12}\operatorname{B_1}(y_i) \\
          + &w_{12}\operatorname{W_2}(y_{i}) +
          w_{13}\operatorname{H_2}(y_{i}) +
          w_{14}\operatorname{B_2}(y_i)\\
          + &... \\
          + &w_{150}\operatorname{W_{49}}(y_{i}) +
          w_{151}\operatorname{H_{49}}(y_i) + w_{152}\operatorname{B_{49}}(y_i)
\end{align}

where $\operatorname{W_k}$, $\operatorname{B_k}$, $\operatorname{H_k}$
are indicator functions for whether a block is labeled as white,
black, or Hispanic. Since every county has a different racial mix, we
need a set indicator variables for each county. These weights affect
how we would label a block if we didn't know anything about it's neighbors.

We estimate all the parameters using the PyStruct
library.\cite{muller_pystruct_2014} In order to evaluate the
robustness of the parameters, we bootstrap a sampling distribution of
the parameters by sampling with replacement from the 49 counties,
estimating the parameters on the sample, recording the results, and
repeating 1000 times. Using the percentile bootstrap, we estimate the
confidence intervals for the parameters.

In Table~\ref{tab:parameters}, we have the parameters for racial
preferences.\footnote{If our formalism, which is typical of the
  literature, the objective is to find the parameters that give the
  observed data the smallest score. PyStruct attempts to find the
  parameters the largest score. This makes no real difference except
  it inverts the signs of the parameters. In order to maintain
  consistency, I invert the signs of PyStruct's parameters in this
  presentation} The values may seem very small, but this is due to the
constraint that $\sqrt{\sum_i^M w_i^2 = 1}$ for the 153
parameters. The racial preference parameters are between one to three
orders of magnitude greater than the city specific bias terms.

If a racial or ethnic preference parameter is negative, then blocks of
that type should tend to be neighbors, all else equal. If a preference
parameter is positive, then blocks of that type should tend to not be
neighbors. The signs of the parameter align with expectations. Each
racial and ethnic groups tend to live among their own group. White and
black blocks, and Hispanic and black groups tend to not to
neighbor. Interestingly, the parameter for white and Hispanic groups
is negative, suggesting that white and Hispanic groups should tend to
neighbor, all else being equal. Only the negative parameter for
``White-White'' and the postive parameter for ``Black-Hispanic'' have
95\% confidence intervals that do not cover 0.

\section*{Conclusion}
This relatively simple analysis demonstrate how we can estimate
parameters that agent-based models have to posit. Instead of moving
from micromotive to macrostructure, the support vector machine lets us
go from macrostructure to micromotives.

This machinery allows us to subject an important class of emergent
sociological problems to empirical tests. The nature and sources of
residential segregation are just one instance of a larger class of
problems where we are interested how network connections effect
discrete outcomes. Other examples can political party affiliations,
music genre preferences, or risk taking among groups of
acquaintances. Of course, when we move from a spatial network to a
social network we must take care to understand how an outcome could
structure the network in addition to how the network could influence
the behavior.

Within the field of urban sociology, the simple models that I have
used to introduced the method can be extended in a number of
ways. They can be used to comprehensively compare the potential
sources of racial segregation, by adding richer variables to the
particular block groups like the average rent and commute times. These
models can also be used to compare the racial preferences between
city, and provide a measure of racial segregation that is scale
free. These measures could be both input our outcome measures for
understanding the causes and effects of racial segregation. 


<<bootstrap>>=
library(RcppCNPy)

load <- function(x) {
    filename = paste0(x, '.npy')
    npyLoad(filename) * -1 #PyStruct's parameters have opposite signs of those typical in the literature
}

ww <- load('white-white')
bw <- load('black-white')
hw <- load("hispanic-white")
bb <- load("black-black")
hb <- load("hispanic-black")
hh <- load("hispanic-hispanic")

edges <- load("edge_param")
@ 


<<parameters, results='asis'>>=
confidence_end_points <- c(0.025, 0.975)

edge_parameters <- rbind(
    "White-White"=c(edges[1,1], quantile(ww, confidence_end_points)),
    "White-Black"=c(edges[2,1], quantile(bw, confidence_end_points)),
    "White-Hispanic"=c(edges[3,1], quantile(hw, confidence_end_points)),
    "Black-Black"=c(edges[2,2], quantile(bb, confidence_end_points)),
    "Black-Hispanic"=c(edges[3,2], quantile(hb, confidence_end_points)),
    "Hispanic-Hispanic"=c(edges[3,3], quantile(hh,confidence_end_points)))


interleave <- function(v1,v2) {
  ord1 <- 2*(1:length(v1))-1
  ord2 <- 2*(1:length(v2))
  c(v1,v2)[order(c(ord1,ord2))]
}

edge_parameters <- interleave(prettyNum(edge_parameters[,1], digits=2), 
                              paste0("(", 
                                     prettyNum(edge_parameters[,2], 
                                               digits=2), 
                                     ", ", 
                                     prettyNum(edge_parameters[,3], 
                                               digits=2), 
                                     ")"))
edge_parameters <- data.frame(names(edge_parameters), edge_parameters)
colnames(edge_parameters) <- c("", "Racial Compatibility")
edge_parameters[,2] <- as.character(edge_parameters[,2])
edge_parameters[1,2] <- paste0(edge_parameters[1,2], "*")
edge_parameters[9,2] <- paste0(edge_parameters[9,2], "*")
      
print(xtable(edge_parameters, align="llr", #caption="Racial Compatibility Parameters\\\\ Numbers in Parenthesis are 95\\% Confidence Intervals (Percentile Bootstrap)\\\\* $P\\leq.05$",
label="tab:parameters"), include.rownames=FALSE)

@ 

\newpage
\begingroup \parindent 0pt
\parskip 2ex
\def\enotesize{\normalsize}
\theendnotes
\endgroup

\newpage
\bibliographystyle{plainnat}
\bibliography{race-potts}



\end{document}

\footnote{We can also use these
  techniques on scoring functions that have a term for the relation
  between the properties of a particular block and the region
  assignment, $\epsilon_i(y_i)$. This is particularly useful when we
  already know something about what meaningful categories should exist.

  \begin{align}
    \operatorname{E}(\mathbf{y}) = \sum_i^N\epsilon_i(y_i) + \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j)
  \end{align}

  For example, if we had a strong, ecological theory of urban space,
  we might want to identify regions like `the red light district,'
  `the zone of industry`, `the central business district.' We would
  want to pay a large penalty if we assigned a block that had lot of
  immigrants to `the central business district` instead of `the
  immigrant neighborhood.'
}

 
<<hist>>= 
par(mfrow=c(3,3))
hist(ww)
hist(bw)
hist(hw)
plot.new()
hist(bb)
hist(hb)
plot.new()
plot.new()
hist(hh)
par(mfrow=c(1,1))

@ 





