\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage{endnotes}
\usepackage{tikz}
\usepackage{adjustbox}
\title{Estimating Emergent Phenomena}
\author{Forest Gregg}

\usetikzlibrary{arrows}
\usetikzlibrary{bayesnet}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\let\footnote=\endnote
\let\cite=\citep

\begin{document}
\maketitle

<<include=FALSE>>=
library(knitr)
library(RPostgreSQL)
library(xtable)
library(RcppCNPy)
opts_chunk$set(
echo=F, fig.width=4.5, fig.height=4.75, fig.path='/home/fgregg/sweave-cache/figs/fig', cache=T, results='hide'
)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
@ 

Sociologists know that large scale patterns can emerge from the
local, mutual influence of individuals. From thought experiments and
simulations, we can see how racial segregation, flocking behavior, or
public order on the street could emerge from individuals making
simple, local adjustments to one another. Yet, while emergentist thinking
has been attractive to sociologists for a long time, there has been no
substantial empirical program to develop and test theories of how local
influence result in global patterns. There are couple of reasons.

First, in many social settings, we cannot statistically identify local
influence. We cannot tell whether Alice jumped off the bridge because
that's what her friend Bob did. Maybe Alice jumped because Alice and
Bob are both bridge-jumping enthusiasts and that's why they are
friends. If we do not have an adequate model of why two people are
interacting, we cannot distinguish the effects of that interaction
from unobserved homophily.\cite{shalizi_homophily_2011}

Second, models with coupled, interdependent units are computationally
harder to estimate than models with independence.  For models with
interdependent, discrete outcomes, the estimation has been beyond the
realm of practical computation.

In this paper, we will focus on this second, technical problem. We
introduce a method called the structured support vector machine, that
can estimate large systems of coupled, discrete variables. With this
method, we will bring observational data to bear the most famous
formal model of emergent order in urban sociology, the Schelling model
of segregation.

\section*{Schelling's Emergent Segregation}
In his 1971 paper ``Dynamic Models of Segregation'' Thomas Schelling
showed that complete residential racial segregation could emerge
through people preferring, even slightly, to live in houses surrounded
mostly by neighbors of their same race.\cite{schelling_dynamic_1971}

In his model, residential locations were divided into a
checkerboard. At the beginning, white and black agents are distributed
randomly across the board. There are more locations than there are
agents, so there are vacant locations. Agents are discontent if a
certain percentage of their neighbors or not of the same color. Time
proceeds in steps.  At every step, all the individuals who were
discontent at the end of the last step move to the nearest location
that makes them content.

Schelling ran a series of variations, changing the number of vacancies,
the number of neighbors that the agent considered relevant, and the
intensity of color preference. In every variant, after enough time
steps, the white and black agents are spatially segregated. 

This was a wholly abstract model. Schelling With the tools Schelling
had, he could not go beyond an extended thought experiment. He had no
way to test this mechanism against other possible explanations, or
even whether the model was consistent with observed residential
segregation in American cities. The necessary techniques for that
became available in the 2000s with the development of a method called
structured support vector machines.

\input{theory_and_math.tex}

\section*{Estimating Racial Preferences}
We now estimate the racial frustration costs from the observed
residential patterns of the 49 most populous American counties (Table~\ref{tab:counties}) .

<<countyTable, results='asis'>>=
library(RPostgreSQL)
con <- DBI::dbConnect(RPostgreSQL::PostgreSQL(), dbname="segregation")

counties <- DBI::dbGetQuery(con, 
    "SELECT name[3] || ', ' || name[4] AS place, 
            SUM(total)/1000000::FLOAT AS population,
            SUM(white)/1000000::FLOAT AS white,
            SUM(black)/1000000::FLOAT  AS black,
            SUM(hispanic)/1000000::FLOAT AS hispanic
     FROM (SELECT REGEXP_SPLIT_TO_ARRAY(\"NAME\", ', ') AS name, 
                  \"B03002_001E\" AS total,
	          \"B03002_003E\" as white, 
	          \"B03002_004E\" as black, 
                  \"B03002_012E\" as hispanic
           FROM race) AS t 
     GROUP BY name[3] || ', ' || name[4] ORDER BY SUM(total) DESC")

names(counties) <- c("County", "Total Population", "White", "Black", "Hispanic")


n_blockgroups <- DBI::dbGetQuery(con, "SELECT COUNT(*) FROM race")
    
print(xtable(counties,
             caption="Populations of 49 Largest Counties (In Millions)",
             label="tab:counties"),
      digits=2,
      include.rownames=FALSE)
@ 

From the 2010-2015 ACS 5-Year Estimates, we pull the population of
Hispanics, Non-Hispanic Whites, and Non-Hispanic African Americans for
every block group in all the counties.\cite{bureau_american_????} For every
block group, we label the block group as Hispanic, White, or Black
based on which group is most common in the block group.  Using the
associated 2014 TIGER/Line Shapefiles, we calculate the adjacency
matrix for the
blockgroups\cite{tiger/line_????,rey_pysal:_2007}. There are \Sexpr{n_blockgroups} block groups in the data.

We'll use these data to estimate the parameters of the following score
function. 

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + \sum_i^N\epsilon_i(y_i)
\end{align}

Where 

\begin{align}
  \epsilon_{i,j} = &w_0\operatorname{WW}(x_{i}, x_{j}) +
  w_1\operatorname{WB}(x_{i}, x_{j}) + 
  w_2\operatorname{WH}(x_{i}, x_{j}) \\
  &+ w_3\operatorname{BB}(x_i, x_j) +
  w_4\operatorname{BH}(x_i, x_j) +
  w_5\operatorname{HH}(x_i, x_j)
\end{align} 

and $\operatorname{WW}$ indicates that block $i$ and $j$ are both
labeled as ``White''; $\operatorname{WB}$ indicates a ``White'' block
and ``Black'' block; $\operatorname{WH}$ indicates indicates a
``White'' block and ``Hispanic'' block; and so on.

In addition, 

\begin{align}
  \epsilon_{i} = w_6 + & w_7\operatorname{W_1}(y_{i}) +  
          w_{10}\operatorname{H_1}(y_{i}) + w_{11} +
          w_{12}\operatorname{B_1}(y_i) \\
          + &w_{12}\operatorname{W_2}(y_{i}) +
          w_{13}\operatorname{H_2}(y_{i}) +
          w_{14}\operatorname{B_2}(y_i)\\
          + &... \\
          + &w_{150}\operatorname{W_{49}}(y_{i}) +
          w_{151}\operatorname{H_{49}}(y_i) + w_{152}\operatorname{B_{49}}(y_i)
\end{align}

where $\operatorname{W_k}$, $\operatorname{B_k}$, $\operatorname{H_k}$
are indicator functions for whether a block is labeled as white,
black, or Hispanic. Since every county has a different racial mix, we
need a set indicator variables for each county. These weights affect
how we would label a block if we didn't know anything about it's neighbors.

We estimate all the parameters using the PyStruct
library.\cite{muller_pystruct_2014} In order to evaluate the
robustness of the parameters, we bootstrap a sampling distribution of
the parameters by sampling with replacement from the 49 counties,
estimating the parameters on the sample, recording the results, and
repeating 1000 times. Using the percentile bootstrap, we estimate the
confidence intervals for the parameters.

In Table~\ref{tab:parameters}, we have the parameters for racial
preferences.\footnote{If our formalism, which is typical of the
  literature, the objective is to find the parameters that give the
  observed data the smallest score. PyStruct attempts to find the
  parameters the largest score. This makes no real difference except
  it inverts the signs of the parameters. In order to maintain
  consistency, I invert the signs of PyStruct's parameters in this
  presentation} The values may seem very small, but this is due to the
constraint that $\sqrt{\sum_i^M w_i^2 = 1}$ for the 153
parameters. The racial preference parameters are between one to three
orders of magnitude greater than the city specific bias terms.

If a racial or ethnic preference parameter is negative, then blocks of
that type should tend to be neighbors, all else equal. If a preference
parameter is positive, then blocks of that type should tend to not be
neighbors. The signs of the parameter align with expectations. Each
racial and ethnic groups tend to live among their own group. White and
black blocks, and Hispanic and black groups tend to not to
neighbor. Interestingly, the parameter for white and Hispanic groups
is negative, suggesting that white and Hispanic groups should tend to
neighbor, all else being equal. Only the negative parameter for
``White-White'' and the postive parameter for ``Black-Hispanic'' have
95\% confidence intervals that do not cover 0.

This relatively simple analysis demonstrate how we can estimate
parameters that Schelling could only posit and how we can use
observational data to test a model of emergent phenomena that before
we could only simulate.

\section*{Conclusion}
The structured support vector machine allows us to subject an
important class of emergent sociological problems to empirical
tests. The nature and sources of residential segregation are just one
instance of a larger class of problems where we are interested how
network connections effect discrete outcomes. Other examples can
political party affiliations, music genre preferences, or risk taking
among groups of acquaintances. Of course, when we move from a spatial
network to a social network we must take care to understand how an
outcome could structure the network in addition to how the network
could influence the behavior.

Within the field of urban sociology, the simple models that I have
used to introduced the method can be extended in a number of
ways. They can be used to comprehensively compare the potential
sources of racial segregation, by adding richer variables to the
particular block groups like the average rent and commute times. These
models can also be used to compare the racial preferences between
city, and provide a measure of racial segregation that is scale
free. These measures could be both input our outcome measures for
understanding the causes and effects of racial segregation. 


<<bootstrap>>=
library(RcppCNPy)

load <- function(x) {
    filename = paste0(x, '.npy')
    npyLoad(filename) * -1 #PyStruct's parameters have opposite signs of those typical in the literature
}

ww <- load('white-white')
bw <- load('black-white')
hw <- load("hispanic-white")
bb <- load("black-black")
hb <- load("hispanic-black")
hh <- load("hispanic-hispanic")

edges <- load("edge_param")
@ 


<<parameters, results='asis'>>=
confidence_end_points <- c(0.025, 0.975)

edge_parameters <- rbind(
    "White-White"=c(edges[1,1], quantile(ww, confidence_end_points)),
    "White-Black"=c(edges[2,1], quantile(bw, confidence_end_points)),
    "White-Hispanic"=c(edges[3,1], quantile(hw, confidence_end_points)),
    "Black-Black"=c(edges[2,2], quantile(bb, confidence_end_points)),
    "Black-Hispanic"=c(edges[3,2], quantile(hb, confidence_end_points)),
    "Hispanic-Hispanic"=c(edges[3,3], quantile(hh, confidence_end_points)))

colnames(edge_parameters) <- c("Estimate", "2.5%", "97.5%")
      
xtable(edge_parameters, digits=-2, caption="Racial Preference Parameters, 95\\% Confidence Intervals (Percentile Bootstrap)",
       label="tab:parameters")

@ 

\newpage
\begingroup \parindent 0pt
\parskip 2ex
\def\enotesize{\normalsize}
\theendnotes
\endgroup

\newpage
\bibliographystyle{plainnat}
\bibliography{race-potts}



\end{document}

\footnote{We can also use these
  techniques on scoring functions that have a term for the relation
  between the properties of a particular block and the region
  assignment, $\epsilon_i(y_i)$. This is particularly useful when we
  already know something about what meaningful categories should exist.

  \begin{align}
    \operatorname{E}(\mathbf{y}) = \sum_i^N\epsilon_i(y_i) + \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j)
  \end{align}

  For example, if we had a strong, ecological theory of urban space,
  we might want to identify regions like `the red light district,'
  `the zone of industry`, `the central business district.' We would
  want to pay a large penalty if we assigned a block that had lot of
  immigrants to `the central business district` instead of `the
  immigrant neighborhood.'
}

 
<<hist>>= 
par(mfrow=c(3,3))
hist(ww)
hist(bw)
hist(hw)
plot.new()
hist(bb)
hist(hb)
plot.new()
plot.new()
hist(hh)
par(mfrow=c(1,1))

@ 
