\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage{endnotes}
\usepackage{tikz}
\usepackage{adjustbox}
\title{From Macrostructure to Micromotives: From Macrostructure to Micromotives: Regression for discrete outomes with spatial autocorrelation}

\author{Forest Gregg}

\usetikzlibrary{arrows}
\usetikzlibrary{bayesnet}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\let\footnote=\endnote
\let\cite=\citep

\begin{document}
\maketitle
\begin{abstract}
  Racial segregation is one of many important social phenomena where a
  discrete outcome at one site depends upon the outcomes of
  neighboring sites. While we have methods to estimate systems where
  there is spatial auto-correlation when outcomes are continuous or
  counts, we have not had a method to estimate parameters of large
  systems of coupled, discrete outcomes in such spatial
  data. Sociologists have explored such systems through agent-based
  modeling, and in the case of racial segregation, have uncovered
  surprising results, such as the fact that racial preferences for
  neighbors could lead to large-scale racial segregation. However, we
  have not had a tools to go from observed global structures to
  estimates of local interactions. In this paper, we introduce a
  strategy for estimating local interactions from large systems of
  coupled, discrete variables: the structured support vector
  machine. Then we demonstrate this approach using detailed
  neighborhood data from the 30 largest U.S. cities to infer local
  housing choices, and outline a range of other social science
  applications
\end{abstract}


<<include=FALSE>>=
library(knitr)
library(RPostgreSQL)
library(xtable)
library(RcppCNPy)
opts_chunk$set(
echo=F, fig.width=4.5, fig.height=4.75, fig.path='/home/fgregg/sweave-cache/figs/fig', cache=T, results='hide'
)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
@ 

<<blocks-intercept, results='asis', cache=FALSE>>=
tract_params <- read.csv('params_blocks.csv', header=FALSE)
names(tract_params) <- c('City', 'N',
                         'Int., Black', 'Median Monthly Rent, Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 'Media Monthly Rent, Hispanic', 
                         'H-B', 'H-W')

no_x <- tract_params[tract_params$"Median Monthly Rent, Black" == 0, 
                     c('City',
                       'Int., Black', 
                       'B-H', 'B-W',
                       'Int., Hispanic', 
                       'H-B', 'H-W')]
xtable(no_x[order(no_x$"B-W"),],
       caption="2010 Blocks, No Housing Costs")
@

<<blocks-rent, results='asis', cache=FALSE>>=
tract_params <- read.csv('params_blocks.csv', header=FALSE)
names(tract_params) <- c('City', 'N', 
                         'Int., Black', 'Median Monthly Rent, Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 'Median Monthly Rent, Hispanic', 
                         'H-B', 'H-W')

with_x <- tract_params[tract_params$"Median Monthly Rent, Black" != 0, 
                       c('City',
                         'Int., Black', 
                         'B-H', 'B-W',
                         'Int., Hispanic', 
                         'H-B', 'H-W')]
xtable(with_x[order(with_x$"B-W"),],
       caption="2010 Blocks, Controlling for Median Monthly Rent")
@ 


Sociologists are interested in a number of social phenomena that have
discrete outcomes and where the outcomes of of adjacent
units are irreducibly interdependent--racial segregation and housing
choice; peer effects on the taking up of risky behavior within a high
school friend group; the relation between a social network and taste
preferences.

There are many, different difficulties in understanding these
phenomena, but they all also share one common hurdle: models of
coupled, discrete outcomes are difficult to estimate. For small
datasets, unless the coupling between variables is very weak, the
sampling error will be tremendous.  For the typical, moderate
sized network data typical of social science, we can't use the most
statistically efficient method because it's too computationally
expensive. The loss of efficiency for a tractable estimation method is
large and increases with the coupling between observations.

As for many statistical problems, if we had more data our problems
would resolve, and we could use statistically inefficient but
computationally practical methods. In some areas of social inquiry,
the digitization of our lives has brought us large, available corpii
of network data like scientific citations or the Enron corpus. Private
companies have even larger datasets available, which are sometimes
made available for research. 

However, many of the largest, best, and most accessible data sources
for network data is geospatial observations, particularly Census
data. Reanalyzing data from this familiar source can provide us with 
a new insights on social processes and help us develop new data for
use with other types of network data.

In this paper, we demonstrate how a method for estimating spatial
dependence of discrete factors can help urban sociologists understand
the classic problem of racial segregation. We will produce
new measures of city level segregation called the ``Schelling index'',
which measures how much local, racist housing preferences are
necessary to produce observed city-level racial segregation. The
statistical tools introduced can be used to comprehensively evaluate
theories of racial segregation that are currently difficult to
directly contrast with empirical data.

\section{Racial Segregation, An Local Approach}
In his classic 1971 paper ..., Thomas Schelling demonstrated that,
within a simplified model of a city, very mild levels of individual
racist housing preferences could lead to stark patterns of racial
segregation. 

In his basic model, there were white, black, and vacant
households. Black and white households are content if the majority of
their neighbors are of the same race. One by one, a discontented
households can move to the nearest vacant household that makes them
content. If we let these simple rules play out over indefinitely, eventually,
black and white households will be segregated into large racially
homogenous regions.

Schelling explored how changing the rules of the simulation and found
that the emergence of large segregation regions was remarkably
robust. All that seems necessary is that at least one race prefer,
even mildly, that most of their neighbors be of the same race and
that the other race not have a positive preference for living next to
neighbors who were mainly of the first race.

Schelling's paper pioneered what now called agent based modeling and
exemplifies all of its typical strengths. He is able set up a set of
rules that are very easy to understand, and then explore
quantitatively, reproducibly, and comprehensively the consequences of
those rules. He can change the almost any rule or configuration and
play the action forward to see the result.

Schelling model also has typifies the usual limitations agent-based
modeling, at least within the sociological literature. Schelling
showed that the local action of households acting on individual
preferences could lead to the residential patterns that resembled the
racial segregation. He did not, and with his tools, could not measure
preferences from observed residential patterns. With no measure, he
could not compare the importance of individual action versus larger
legal and economic forces to maintaining racial segregation. He showed
that individual, local action could lead to racial segregation, but
couldn't show that this mechanism actually mattered in the observed
segregation of American cities.

As we discuss in the next section, we can now get a measure
of racial preferences from actual housing patterns. We can get a
``Schelling measure'' though we will have to give up some of the
original flexibility of his model.

\section{Racial Segregation, an Autocorrelated Approach}
Schelling's model has a sharp discontinuity that is inconvenient to
work with and not critical to his results. If a household is
surrounded by a majority of neighbors of the same race it will never
move, but if it is in the minority it will always attempt to move. 

Instead, allow the proportion of neighbors of different races to
smoothly affect the probability of a house being occupied by a white
or black household. We can then capture the Schelling dynamics with a
very familiar probabilistic model.

First we introduce a little terminology. Let $y_i$ be the race of the
household in the $i$th house. If the $i$th house is occupied by a
white household, $y_i$ takes a value of $1$, and if it is occupied by
a black household, $y_i$ takes a value of $0$. Let $\rho$ be a
measure of racial preference; let $\operatorname{N}(i)$ be the set of
indices $j$ which neighbor the $i$ house; and let $\mathcal{N}_i$ be the
number of neighbors of the $i$th house. 

We can write the log odds of house being occupied by a white household as

\begin{align}
  \log\frac{\Pr(y_i = 1 \mid \{y_j : j \neq i\})}{\Pr(y_i = 0 \mid \{y_j : j \neq i\})} = \rho\frac{1}{\mathcal{N}_i}\sum_{j \in \operatorname{N}(i)}y_j
\end{align}

This is almost the workhorse model for logistic regression, except
that the outcome $y_i$ is interdependent on all the other outcomes of
neighboring houses.  That interdependence is also called
autocorrelation, and this statistical model is known as logistic
regression with spatial correlation or the autologistic model.

Estimating this model poses many challenges, but before we turn to
those, we should acknowledge what we give up when we move from the
agent-based model to this formal, probabilistic model. 

First, in agent based model, we can freely choose how neighbors effect
the focal outcome, but the probabilistic model assumes that each
neighbors contributes to the focal outcome independently and
additively, but see (citations for higher order cliques).

Second, we cannot have separate parameters for the racial preferences
of black households and white households. From survey research, we
know that black and white households do differ in their preferences
for the races of their neighbors. Our model cannot recover those
preferences. Instead, $\rho$ summarizes the interaction of those
preferences into a single correlation-like measure.

\section{Estimation}
Our classic methods, whether frequentist or Bayesian, for estimating
parameters depend upon being able to calculate the likelihood of the
data. However, while the spatial model looks similar to the logistic
regression models, the full data likelihood turns out to be
intractable. The likelihood depends not just on the observed patterns
of racial segregation, but every single possible pattern of racial
segregation.

Let $\mathbf{z}$ be a possible pattern of racial segregation
and $\zeta$ be the set of all possible patterns.

\begin{align}
  Z = \sum_{\mathbf{z} \in \zeta} \exp\Big({\sum_i^N(\beta_0z_i + ... + \beta_dx_dz_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}z_iz_j}\Big)
\end{align}
and the full data log likelihood of an observed pattern of racial
segregation, $\mathbf{y}$, is 

\begin{align}
  \log\Pr(\mathbf{\beta}, \rho \mid \mathbf{y}) = (\sum_i^M(\beta_0y_i  +
  ... + \beta_dx_dy_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j)- Z
\end{align}

For two race and $N$ sites there are $2^N$ possible patterns in
$\zeta$. So, with just 30 sites, the calculating the require takes
over a billion arithmetic operations. For all but the smallest data
sets, calculating the likelihood is computationally intractable.

If we cannot calculate the likelihood, we cannot maximize the
likelihood to estimate parameters. There have been two main responses
to this computational barrier, pseudolikelihood, MCMC approxmiation,
and simulation.

\subsection*{Psuedolikelihood}
Originally proposed by Besag, the pseudolikelihood is

\begin{align}
  p(\boldsymbol{\theta} \mid \mathbf{y}) = \prod_i^N\Pr(y_i \mid \{y_j : j \neq i\})
\end{align}

This is effectively the 
so for our model, the log pseudolikelihood is

\begin{align}
  \log p(\boldsymbol{\beta}, \rho \mid \mathbf{y}) = &\sum_i^M(\beta_0y_i +
  ... + \beta_dx_{di}y_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j\\&- \sum_i^M\log(1 + e^{\beta_0 +
  ... + \beta_dx_{id} + \rho\sum_{j \in \operatorname{N}(i)}y_j})
\end{align}

Like the maximum likelihood estimate, the maximum pseudolikelihood
estimate is a consistent estimator. That is, as the, the number of
sites $M$ goes to infinity the maximum pseudolikelihood parameter
estimates $\hat{\boldsymbol{\theta}}$ converges to the true parameter
$\boldsymbol{\theta}$. This convergence is slower than the maximum
likelihood estimator, and the loss of efficiency increases with the
strength of the interdependence between sites\cite{Cresside 488}. The
bias of the estimator has not been fully characterized \cite{erg}.

Social scientists have used psuedolikelihood methods to study a
problem that looks like the near opposite of the problem of racial
segregation. Given an observed social network of nodes and edges, can
we estimate the parameters that determine the likelihood that two
nodes will share an edge. In the problem of racial segregation, we
want to know how the races of the neighbors of site $i$ will affect
the race of the household at site $i$. In this social network problem,
the task is to estimate how the attributes of nodes $j$ and $k$ will
make it more likely that they will be ``neighbors'', i.e. share a tie.

Ultimately, the problem can take the same mathematical form if the
likelihood that node $j$ and $k$ are connected depends upon the number
of other ties that both $j$ and $k$ have. Let $y_{ij}$ take a value of
$1$ if there is a tie between node $i$ and node $j$ and $0$ otherwise,
then we could express this interdependency as follows

\begin{align}
  \log\frac{\Pr(y_{ij} = 1 \mid \{y_{kl} : k \neq i\ \text{and } l \neq
    j\})}{\Pr(y_{ij} = 0 \mid \{y_{kl} : k \neq i\ \text{and } l \neq j\})} =
  \beta_0 + \beta_1x_{0ij} + ... + \beta_dx_{dij} + \rho(\sum_{l \neq
    j}y_{il} + \sum_{k \neq i}y_{kj})
\end{align}



Within sociology, the most famous example is the emergence model of
racial segregation, which explains stark, city wide patterns of racial
segregation as the result of individual households acting on mild
preferences for same-race neighbors. In his 1971 paper, Thomas
Schelling introduced a model with black and white agents, endowed them
with preferences, set rules for when and how each agent could move,
and ran the simulation forward.\cite{schelling_dynamic_1971} When he
set the preferences of the agents to even mildly prefer to be
surrounded by agents of the same color, the black and white agents
would eventually segregate into distinct neighborhoods. These large
scale patterns looked quite similar to patterns of actual, racial
segregation in contemporary American cities.

Since his pioneering work, researchers have improved substantially on
his method. In Elizabeth Bruch's exemplary, recent paper on how within
group income inequality and between inequality can effect racial
segregation, the preferences of the agents are informed by research
into individual racial preferences, the agents move over a realistic
geographic space and not an abstract grid, and she uses the simulation
to derive specific, suprising associations that should appear in
aggregate data and which she validates
empirically.\cite{bruch_how_2014}

Agent-based modeling lets researchers explore how local actions could
lead to global patterns. Ideally, we would also be able to go in the
opposite direction. We would derive local effects from global patterns
and test whether those local effects are consistent with theory and
other evidence. While the typical model sociologists would use to
estimate the local effects are not tractable, in this paper we
introduce an estimation technique that can be used---the structured support
vector machine.




how city-wide racial segregation can emerge from
individual households with slight racists preferences for neighbors.



Before introducing the structured support vector machine, we should
examine why our classic approaches will not work.
of 















For very small systems, we
could use a statistically efficient estimation technique, but we would
be 

We have not had
efficient techniques for estimating systems that have both discrete
outcomes and interdependence between outcomes. 


More precisely, we have
had two problems.



 



. Sociologists have not 
effective method to estimate large system with both discrete outcomes
and interdependence between adjacent units. While we now commonly
to estimate spatially auto-correlated systems with continuous or count
outcomes, autocorrelated discrete outcomes are intractable to classic
estimating approaches.

The recent development of a method called the Structured Support
Vector Machine provides a means address these systems. While this
method does not provide all the familiar benefits of our common
statistical tools, we are now able to provide point estimates and
confidence intervals for wide set of previously inaccessible questions.

In this paper, we introduce this method using as an example the
classic problem of how city-wide racial segregation can emerge from
individual households with slight racists preferences for neighbors.
Before introducing the structured support vector machine, we should
examine why our classic approaches will not work.

\subsection*{Modeling local interaction}
Briefly, there are two broad types of explanations for racial
segregation in American cities. First, different areas have different
benefits and costs to the people who live there, like average rent,
commute times, or access to lovely parks. If different racial groups
differ in either their ability to pay for housing or in their tastes,
they will tend to sort into different areas. Second, if individuals
prefer to live near neighbors of the same race or, equivalently, don't
want to live near neighbors of different races, then large scale
segregation can emerge, even if there's no difference, beforehand,
between places or between the wealth or tastes of the racial groups.

In order to compare the power of these two explanations, we might want
to see which factor explained more of observed residential racial
segregation in American cities. For the moment, assume that we live in
a black and white world.

If we already knew that neighbors do not matter, we could use logistic
regression to model how the characteristics of each site effect the
race of the person at that site.  Let
$y_i$ indicate the race of a person living a the $i$th site and have
$y_i$ take a value of $1$ if the person is black and $0$ if
white. Each site $i$ has a set of $D$ observed attributes $(x_0, x_1,
... x_{D-1}, x_D)$. The parameters $(\beta_0, \beta_1,
... \beta_{D-1}, \beta_D)$ determine the influence of the attributes
of the $i$th site on race of the person living at that site.

\begin{align}
  \log\frac{\Pr(y_i = 1)}{\Pr(y_i = 0)} = \beta_0 + \beta_1x_{0i} + ... + \beta_dx_d
\end{align}

In order to bring in the effect of neighbors, we can have a model like 

\begin{align}
  \log\frac{\Pr(y_i = 1 \mid \{y_j : j \neq i\})}{\Pr(y_i = 0 \mid \{y_j : j \neq i\})} = \beta_0 + \beta_1x_{0i} + ... + \beta_dx_d + \rho\sum_{j \in \operatorname{N}(i)}y_j
\end{align}
The parameter $\rho$ is the effect of the
race of neighbors of site $i$ on the race of the person at site
$i$. $\operatorname{N}(i)$ is a function that returns the indices of
the neighbors of the $i$th site.

If we could estimate this second equation, we could compare the two main
proposed mechanisms for racial segregation. If $\rho$ is a
persistent and consequential component of the model, that would be
strong evidence for racial segregation as a continued, emergent result
of individual racism.

\section*{Estimation}
Our classic methods, whether frequentist or Bayesian, for estimating
parameters depend upon being able to calculate the likelihood of the
data. However, while the spatial model looks similar to the logistic
regression models, the full data likelihood turns out to be
intractable. The likelihood depends not just on the observed patterns
of racial segregation, but every single possible pattern of racial
segregation.

Let $\mathbf{z}$ be a possible pattern of racial segregation
and $\zeta$ be the set of all possible patterns.

\begin{align}
  Z = \sum_{\mathbf{z} \in \zeta} \exp\Big({\sum_i^N(\beta_0z_i + ... + \beta_dx_dz_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}z_iz_j}\Big)
\end{align}
and the full data log likelihood of an observed pattern of racial
segregation, $\mathbf{y}$, is 

\begin{align}
  \log\Pr(\mathbf{\beta}, \rho \mid \mathbf{y}) = (\sum_i^M(\beta_0y_i  +
  ... + \beta_dx_dy_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j)- Z
\end{align}

For two race and $N$ sites there are $2^N$ possible patterns in
$\zeta$. So, with just 30 sites, the calculating the require takes
over a billion arithmetic operations. For all but the smallest data
sets, calculating the likelihood is computationally intractable.

If we cannot calculate the likelihood, we cannot maximize the
likelihood to estimate parameters. There have been two main responses
to this computational barrier, pseudolikelihood, MCMC approxmiation,
and simulation.

\subsection*{Psuedolikelihood}
Originally proposed by Besag, the pseudolikelihood is

\begin{align}
  p(\boldsymbol{\theta} \mid \mathbf{y}) = \prod_i^N\Pr(y_i \mid \{y_j : j \neq i\})
\end{align}

This is effectively the 
so for our model, the log pseudolikelihood is

\begin{align}
  \log p(\boldsymbol{\beta}, \rho \mid \mathbf{y}) = &\sum_i^M(\beta_0y_i +
  ... + \beta_dx_{di}y_i) + \rho\sum_i\sum_{j \in \operatorname{N}(i)}
  y_i,y_j\\&- \sum_i^M\log(1 + e^{\beta_0 +
  ... + \beta_dx_{id} + \rho\sum_{j \in \operatorname{N}(i)}y_j})
\end{align}

Like the maximum likelihood estimate, the maximum pseudolikelihood
estimate is a consistent estimator. That is, as the, the number of
sites $M$ goes to infinity the maximum pseudolikelihood parameter
estimates $\hat{\boldsymbol{\theta}}$ converges to the true parameter
$\boldsymbol{\theta}$. This convergence is slower than the maximum
likelihood estimator, and the loss of efficiency increases with the
strength of the interdependence between sites\cite{Cresside 488}. The
bias of the estimator has not been fully characterized \cite{erg}.

Social scientists have used psuedolikelihood methods to study a
problem that looks like the near opposite of the problem of racial
segregation. Given an observed social network of nodes and edges, can
we estimate the parameters that determine the likelihood that two
nodes will share an edge. In the problem of racial segregation, we
want to know how the races of the neighbors of site $i$ will affect
the race of the household at site $i$. In this social network problem,
the task is to estimate how the attributes of nodes $j$ and $k$ will
make it more likely that they will be ``neighbors'', i.e. share a tie.

Ultimately, the problem can take the same mathematical form if the
likelihood that node $j$ and $k$ are connected depends upon the number
of other ties that both $j$ and $k$ have. Let $y_{ij}$ take a value of
$1$ if there is a tie between node $i$ and node $j$ and $0$ otherwise,
then we could express this interdependency as follows

\begin{align}
  \log\frac{\Pr(y_{ij} = 1 \mid \{y_{kl} : k \neq i\ \text{and } l \neq
    j\})}{\Pr(y_{ij} = 0 \mid \{y_{kl} : k \neq i\ \text{and } l \neq j\})} =
  \beta_0 + \beta_1x_{0ij} + ... + \beta_dx_{dij} + \rho(\sum_{l \neq
    j}y_{il} + \sum_{k \neq i}y_{kj})
\end{align}

However, even with pseudolikelihood
approach, the 






is close to the inverse of the problem of racial
segregation: given a set of nodes with attributes, what is the
probability that there will be an edge between those nodes.
networks\cite{strauss,ikeda,frank,wasserman pattison,wasserman
  robbins,van Dujin}

\subsection*{MCMC Estimate of the Maximum Likelihood Estimate}
Using Markov Chain Monte Carlo methods, it is possible to approximate
the likelihood and estimate the maximum likelihood estimate of
$\boldsymbol{\theta}$. While there has been good progress in refining
  MCMC methods for estimating these types of models \cite{geyer, moller,
    hughes}, the immense number of possible patterns make MCMC methods
  intractable for data with more than a thousand sites.

As Hughe and his coauthors point out, while MCMC methods become
intractable as the number of sites increase, a large number of sites
should increase our confidence in the pseudolikelihood estimator. In a
simulation study, Hughes et.al found that accurate estimation was not
possible with less than 900 site, regardless of the method, but that
at above this size that the pseudolikelihood estimator performed as
well as the MCMC maximum likelihood estimator.

\subsection*{Simulation}
Social scientists have largely 

Social scientists believe that many large scale social patterns 
emerge from individuals making local adjustments to one
another. Historically, these types of emergence theories have been,
historically, hard to test quantitatively. The irreducible
interdependence of outcomes presents technical difficulties that made
estimation of these types of models practically impossible. With
increased computing power and advances in statistical method, we now
have techniques to model coupled outcomes when the outcomes are
continuous or counts. However, we still have not had a means to
estimate models of coupled outcomes where the outcomes are discrete.

Fortunately, this has not prevented us from exploring these types of
models altogether. While we have not been able to estimate local
effects from observed global patterns, we have been able to explore
how local effects can shape global patterns, using agent-based
modeling.

Within sociology, the most famous example is the emergence model of
racial segregation, which explains stark, city wide patterns of racial
segregation as the result of individual households acting on mild
preferences for same-race neighbors. In his 1971 paper, Thomas
Schelling introduced a model with black and white agents, endowed them
with preferences, set rules for when and how each agent could move,
and ran the simulation forward.\cite{schelling_dynamic_1971} When he
set the preferences of the agents to even mildly prefer to be
surrounded by agents of the same color, the black and white agents
would eventually segregate into distinct neighborhoods. These large
scale patterns looked quite similar to patterns of actual, racial
segregation in contemporary American cities.

Since his pioneering work, researchers have improved substantially on
his method. In Elizabeth Bruch's exemplary, recent paper on how within
group income inequality and between inequality can effect racial
segregation, the preferences of the agents are informed by research
into individual racial preferences, the agents move over a realistic
geographic space and not an abstract grid, and she uses the simulation
to derive specific, suprising associations that should appear in
aggregate data and which she validates
empirically.\cite{bruch_how_2014}

Agent-based modeling lets researchers explore how local actions could
lead to global patterns. Ideally, we would also be able to go in the
opposite direction. We would derive local effects from global patterns
and test whether those local effects are consistent with theory and
other evidence. While the typical model sociologists would use to
estimate the local effects are not tractable, in this paper we
introduce an estimation technique that can be used---the structured support
vector machine.


The full data likelihood for this model, has normalizing constant that
depends upon every possible way that blacks and whites could live on
the i sites.

In, the likelihood equation for this model of this model (Equation
\ref{equation:likelihood}), there is a sum that depends on every
possible way that blacks and whites could live at all possible sites
.\cite{cressie_statistics_1993} If there are $M$ locations, there are
$2^M$ possible residential patterns for the two races. For all but the
very smallest number of sites, this sum cannot be practically
computed.
\begin{align}
\log(L(\mathbf{y} | b_0,\rho)) = b_0\sum_iy_i +
\frac{\rho}{2}\sum_{i,j}^\mathcal{N}y_iy_j -
\sum_{z_i,...,z_D}e^{b_0\sum_ky_k + \frac{\rho}{2}\sum_{k,l}^\mathcal{N}y_ky_l}
\label{equation:likelihood}
\end{align}
Where $\mathcal{N}$ is the set of pairs of indices of neighboring
blocks and $z_i,...,z_D$ denotes every single possible way that blacks
and whites could live on the $M$ sites.  

This enormous sum makes maximum likelihood estimation or Bayesian
techniques impossible for models with even a moderate number of
locations. The structured support vector machine lets us solve a very
similar problem, while avoiding that intractable sum.

\end{document}

% \input{theory_and_math.tex}


% \section*{Estimating Racial Preferences}
% We now estimate the racial frustration costs from the observed
% residential patterns of the 49 most populous American counties (Table~\ref{tab:counties}) .

% <<countyTable, results='asis'>>=
% library(RPostgreSQL)
% con <- DBI::dbConnect(RPostgreSQL::PostgreSQL(), dbname="segregation")

% counties <- DBI::dbGetQuery(con, 
%     "SELECT name[3] || ', ' || name[4] AS place, 
%             SUM(total)/1000000::FLOAT AS population,
%             SUM(white)/1000000::FLOAT AS white,
%             SUM(black)/1000000::FLOAT  AS black,
%             SUM(hispanic)/1000000::FLOAT AS hispanic
%      FROM (SELECT REGEXP_SPLIT_TO_ARRAY(\"NAME\", ', ') AS name, 
%                   \"B03002_001E\" AS total,
% 	          \"B03002_003E\" as white, 
% 	          \"B03002_004E\" as black, 
%                   \"B03002_012E\" as hispanic
%            FROM race) AS t 
%      GROUP BY name[3] || ', ' || name[4] ORDER BY SUM(total) DESC")

% names(counties) <- c("County", "Total Population", "White", "Black", "Hispanic")


% n_blockgroups <- DBI::dbGetQuery(con, "SELECT COUNT(*) FROM race")
    
% print(xtable(counties,
%              caption="Populations of 49 Largest Counties (In Millions)",
%              label="tab:counties"),
%       digits=2,
%       include.rownames=FALSE)
% @ 

% From the 2010-2015 ACS 5-Year Estimates, we pull the population of
% Hispanics, Non-Hispanic Whites, and Non-Hispanic African Americans for
% every block group in all the counties.\cite{bureau_american_????} For every
% block group, we label the block group as Hispanic, White, or Black
% based on which group is most common in the block group.  Using the
% associated 2014 TIGER/Line Shapefiles, we calculate the adjacency
% matrix for the
% blockgroups\cite{tiger/line_????,rey_pysal:_2007}. There are \Sexpr{n_blockgroups} block groups in the data.

% We'll use these data to estimate the parameters of the following score
% function. 

% \begin{align}
% \operatorname{E}(\mathbf{y}) = \sum_{<i
%   j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + \sum_i^N\epsilon_i(y_i)
% \end{align}

% Where 

% \begin{align}
%   \epsilon_{i,j} = &w_0\operatorname{WW}(x_{i}, x_{j}) +
%   w_1\operatorname{WB}(x_{i}, x_{j}) + 
%   w_2\operatorname{WH}(x_{i}, x_{j}) \\
%   &+ w_3\operatorname{BB}(x_i, x_j) +
%   w_4\operatorname{BH}(x_i, x_j) +
%   w_5\operatorname{HH}(x_i, x_j)
% \end{align} 

% and $\operatorname{WW}$ indicates that block $i$ and $j$ are both
% labeled as ``White''; $\operatorname{WB}$ indicates a ``White'' block
% and ``Black'' block; $\operatorname{WH}$ indicates indicates a
% ``White'' block and ``Hispanic'' block; and so on.

% In addition, 

% \begin{align}
%   \epsilon_{i} = w_6 + & w_7\operatorname{W_1}(y_{i}) +  
%           w_{10}\operatorname{H_1}(y_{i}) + w_{11} +
%           w_{12}\operatorname{B_1}(y_i) \\
%           + &w_{12}\operatorname{W_2}(y_{i}) +
%           w_{13}\operatorname{H_2}(y_{i}) +
%           w_{14}\operatorname{B_2}(y_i)\\
%           + &... \\
%           + &w_{150}\operatorname{W_{49}}(y_{i}) +
%           w_{151}\operatorname{H_{49}}(y_i) + w_{152}\operatorname{B_{49}}(y_i)
% \end{align}

% where $\operatorname{W_k}$, $\operatorname{B_k}$, $\operatorname{H_k}$
% are indicator functions for whether a block is labeled as white,
% black, or Hispanic. Since every county has a different racial mix, we
% need a set indicator variables for each county. These weights affect
% how we would label a block if we didn't know anything about it's neighbors.

% We estimate all the parameters using the PyStruct
% library.\cite{muller_pystruct_2014} In order to evaluate the
% robustness of the parameters, we bootstrap a sampling distribution of
% the parameters by sampling with replacement from the 49 counties,
% estimating the parameters on the sample, recording the results, and
% repeating 1000 times. Using the percentile bootstrap, we estimate the
% confidence intervals for the parameters.

% In Table~\ref{tab:parameters}, we have the parameters for racial
% preferences.\footnote{If our formalism, which is typical of the
%   literature, the objective is to find the parameters that give the
%   observed data the smallest score. PyStruct attempts to find the
%   parameters the largest score. This makes no real difference except
%   it inverts the signs of the parameters. In order to maintain
%   consistency, I invert the signs of PyStruct's parameters in this
%   presentation} The values may seem very small, but this is due to the
% constraint that $\sqrt{\sum_i^M w_i^2 = 1}$ for the 153
% parameters. The racial preference parameters are between one to three
% orders of magnitude greater than the city specific bias terms.

% If a racial or ethnic preference parameter is negative, then blocks of
% that type should tend to be neighbors, all else equal. If a preference
% parameter is positive, then blocks of that type should tend to not be
% neighbors. The signs of the parameter align with expectations. Each
% racial and ethnic groups tend to live among their own group. White and
% black blocks, and Hispanic and black groups tend to not to
% neighbor. Interestingly, the parameter for white and Hispanic groups
% is negative, suggesting that white and Hispanic groups should tend to
% neighbor, all else being equal. Only the negative parameter for
% ``White-White'' and the postive parameter for ``Black-Hispanic'' have
% 95\% confidence intervals that do not cover 0.

% \section*{Conclusion}
% This relatively simple analysis demonstrate how we can estimate
% parameters that agent-based models have to posit. Instead of moving
% from micromotive to macrostructure, the support vector machine lets us
% go from macrostructure to micromotives.

% This machinery allows us to subject an important class of emergent
% sociological problems to empirical tests. The nature and sources of
% residential segregation are just one instance of a larger class of
% problems where we are interested how network connections effect
% discrete outcomes. Other examples can political party affiliations,
% music genre preferences, or risk taking among groups of
% acquaintances. Of course, when we move from a spatial network to a
% social network we must take care to understand how an outcome could
% structure the network in addition to how the network could influence
% the behavior.

% Within the field of urban sociology, the simple models that I have
% used to introduced the method can be extended in a number of
% ways. They can be used to comprehensively compare the potential
% sources of racial segregation, by adding richer variables to the
% particular block groups like the average rent and commute times. These
% models can also be used to compare the racial preferences between
% city, and provide a measure of racial segregation that is scale
% free. These measures could be both input our outcome measures for
% understanding the causes and effects of racial segregation. 


% <<bootstrap>>=
% library(RcppCNPy)

% load <- function(x) {
%     filename = paste0(x, '.npy')
%     npyLoad(filename) * -1 %PyStruct's parameters have opposite signs of those typical in the literature
% }

% ww <- load('white-white')
% bw <- load('black-white')
% hw <- load("hispanic-white")
% bb <- load("black-black")
% hb <- load("hispanic-black")
% hh <- load("hispanic-hispanic")

% edges <- load("edge_param")
% @ 


% <<parameters, results='asis'>>=
% confidence_end_points <- c(0.025, 0.975)

% edge_parameters <- rbind(
%     "White-White"=c(edges[1,1], quantile(ww, confidence_end_points)),
%     "White-Black"=c(edges[2,1], quantile(bw, confidence_end_points)),
%     "White-Hispanic"=c(edges[3,1], quantile(hw, confidence_end_points)),
%     "Black-Black"=c(edges[2,2], quantile(bb, confidence_end_points)),
%     "Black-Hispanic"=c(edges[3,2], quantile(hb, confidence_end_points)),
%     "Hispanic-Hispanic"=c(edges[3,3], quantile(hh,confidence_end_points)))


% interleave <- function(v1,v2) {
%   ord1 <- 2*(1:length(v1))-1
%   ord2 <- 2*(1:length(v2))
%   c(v1,v2)[order(c(ord1,ord2))]
% }

% edge_parameters <- interleave(prettyNum(edge_parameters[,1], digits=2), 
%                               paste0("(", 
%                                      prettyNum(edge_parameters[,2], 
%                                                digits=2), 
%                                      ", ", 
%                                      prettyNum(edge_parameters[,3], 
%                                                digits=2), 
%                                      ")"))
% edge_parameters <- data.frame(names(edge_parameters), edge_parameters)
% colnames(edge_parameters) <- c("", "Racial Compatibility")
% edge_parameters[,2] <- as.character(edge_parameters[,2])
% edge_parameters[1,2] <- paste0(edge_parameters[1,2], "*")
% edge_parameters[9,2] <- paste0(edge_parameters[9,2], "*")
      
% print(xtable(edge_parameters, align="llr", %caption="Racial Compatibility Parameters\\\\ Numbers in Parenthesis are 95\\% Confidence Intervals (Percentile Bootstrap)\\\\* $P\\leq.05$",
% label="tab:parameters"), include.rownames=FALSE)

% @ 

% \newpage
% \begingroup \parindent 0pt
% \parskip 2ex
% \def\enotesize{\normalsize}
% \theendnotes
% \endgroup

% \newpage
% \bibliographystyle{plainnat}
% \bibliography{race-potts}



% \end{document}

% \footnote{We can also use these
%   techniques on scoring functions that have a term for the relation
%   between the properties of a particular block and the region
%   assignment, $\epsilon_i(y_i)$. This is particularly useful when we
%   already know something about what meaningful categories should exist.

%   \begin{align}
%     \operatorname{E}(\mathbf{y}) = \sum_i^N\epsilon_i(y_i) + \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j)
%   \end{align}

%   For example, if we had a strong, ecological theory of urban space,
%   we might want to identify regions like `the red light district,'
%   `the zone of industry`, `the central business district.' We would
%   want to pay a large penalty if we assigned a block that had lot of
%   immigrants to `the central business district` instead of `the
%   immigrant neighborhood.'
% }

 
% <<hist>>= 
% par(mfrow=c(3,3))
% hist(ww)
% hist(bw)
% hist(hw)
% plot.new()
% hist(bb)
% hist(hb)
% plot.new()
% plot.new()
% hist(hh)
% par(mfrow=c(1,1))

% @ 





