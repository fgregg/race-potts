\documentclass{article}
\usepackage{amsmath}
\usepackage{url}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage{endnotes}
\usepackage{tikz}
\usepackage{adjustbox}
\title{From Macrostructure to Micromotives: Inferring plausible racial preferences from global neighborhood segregation}
\author{Forest Gregg}

\usetikzlibrary{arrows}
\usetikzlibrary{bayesnet}

\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\let\footnote=\endnote
\let\cite=\citep

\begin{document}
\maketitle
\begin{abstract}
With agent based modeling, sociologists have explored how
  local actions can lead to surprising global structures, like mild
  racial preferences for neighbors can lead to large-scale racial
  segregation. We have not, however, had effective tools to go from
  observed global structures to estimates of local interactions. In
  this paper, we introduce a strategy for estimating local
  interactions from large systems of coupled, discrete variables: the
  structured support vector machine. Then we demonstrate this approach
  using detailed neighborhood data from the 30 largest U.S. cities to
  infer local housing choices, and outline a range of other social
  science applications
\end{abstract}


<<include=FALSE>>=
library(knitr)
library(RPostgreSQL)
library(xtable)
library(RcppCNPy)
opts_chunk$set(
echo=F, fig.width=4.5, fig.height=4.75, fig.path='/home/fgregg/sweave-cache/figs/fig', cache=T, results='hide'
)
knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})
@ 

Social scientists believe that many large scale social patterns 
emerge from individuals making local adjustments to one
another. Historically, these types of emergence theories have been,
historically, hard to test quantitatively. The irreducible
interdependence of outcomes presents technical difficulties that made
estimation of these types of models practically impossible. With
increased computing power and advances in statistical method, we now
have techniques to model coupled outcomes when the outcomes are
continuous or counts. However, we still have not had a means to
estimate models of coupled outcomes where the outcomes are discrete.

Fortunately, this has not prevented us from exploring these types of
models altogether. While we have not been able to estimate local
effects from observed global patterns, we have been able to explore
how local effects can shape global patterns, using agent-based
modeling.

Within sociology, the most famous example is the emergence model of
racial segregation, which explains stark, city wide patterns of racial
segregation as the result of individual households acting on mild
preferences for same-race neighbors. In his 1971 paper, Thomas
Schelling introduced a model with black and white agents, endowed them
with preferences, set rules for when and how each agent could move,
and ran the simulation forward.\cite{schelling_dynamic_1971} When he
set the preferences of the agents to even mildly prefer to be
surrounded by agents of the same color, the black and white agents
would eventually segregate into distinct neighborhoods. These large
scale patterns looked quite similar to patterns of actual, racial
segregation in contemporary American cities.

Since his pioneering work, researchers have improved substantially on
his method. In Elizabeth Bruch's exemplary, recent paper on how within
group income inequality and between inequality can effect racial
segregation, the preferences of the agents are informed by research
into individual racial preferences, the agents move over a realistic
geographic space and not an abstract grid, and she uses the simulation
to derive specific, suprising associations that should appear in
aggregate data and which she validates
empirically.\cite{bruch_how_2014}

Agent-based modeling lets researchers explore how local actions could
lead to global patterns. Ideally, we would also be able to go in the
opposite direction. We would derive local effects from global patterns
and test whether those local effects are consistent with theory and
other evidence. While the typical model sociologists would use to
estimate the local effects are not tractable, in this paper we
introduce an estimation technique that can be used---the structured support
vector machine.

\subsection*{Autologistic Regression}
Using our typical tools, we might model write down a model of how
local interaction leads to racial segregation like like Equation
\ref{equation:autolog} (two-race case). This model is called the
autologistic model and is very similar to the typical problem of
logistic regression; $y_i$ is the race of the people at the $i$th
location, and we code one race as 1 and the other as 0.
\begin{align}
\log\frac{\Pr(y_i = 1)}{\Pr(y_i = 0)} = w_0 + w_1\sum_{j \in \operatorname{N}(i)}y_j
\label{equation:autolog}
\end{align}
Where $\operatorname{N}(i)$ is a function that returns the indices of
the neighbors of the $i$th location. 

If we could estimate this model, $w_1$ would measure the strength of
same-race preferences. From an observed global pattern, we could
derive a measure of local interaction.

Unfortunately, in the likelihood equation in the likelihood
(Equation \ref{equation:likelihood}), there is a sum that depends on
every single residential pattern.\cite{cressie_statistics_1993} If
there are $M$ locations, there are $2^M$ possible residential patterns. For
all but the very smallest number of locations, this sum cannot be
practically computed.
\begin{align}
\log(L(\mathbf{y} | w_0,w_1)) = w_0\sum_iy_i +
\frac{w_1}{2}\sum_{i,j}^\mathcal{N}y_iy_j -
\sum_{z_i,...,z_D}e^{w_0\sum_ky_k + \frac{w_1}{2}\sum_{k,l}^\mathcal{N}y_ky_l}
\label{equation:likelihood}
\end{align}
Where $\mathcal{N}$ is the set of pairs of indices of neighboring
blocks and $z_i,...,z_D$ denotes every single possible configuration.  

This enormous sum makes maximum likelihood estimation or Bayesian
techniques impossible for models with even a moderate number of
locations. The structured support vector machine lets us solve a very
similar problem, while avoiding that intractable sum.

\input{theory_and_math.tex}

\section*{Estimating Racial Preferences}
We now estimate the racial frustration costs from the observed
residential patterns of the 49 most populous American counties (Table~\ref{tab:counties}) .

<<countyTable, results='asis'>>=
library(RPostgreSQL)
con <- DBI::dbConnect(RPostgreSQL::PostgreSQL(), dbname="segregation")

counties <- DBI::dbGetQuery(con, 
    "SELECT name[3] || ', ' || name[4] AS place, 
            SUM(total)/1000000::FLOAT AS population,
            SUM(white)/1000000::FLOAT AS white,
            SUM(black)/1000000::FLOAT  AS black,
            SUM(hispanic)/1000000::FLOAT AS hispanic
     FROM (SELECT REGEXP_SPLIT_TO_ARRAY(\"NAME\", ', ') AS name, 
                  \"B03002_001E\" AS total,
	          \"B03002_003E\" as white, 
	          \"B03002_004E\" as black, 
                  \"B03002_012E\" as hispanic
           FROM race) AS t 
     GROUP BY name[3] || ', ' || name[4] ORDER BY SUM(total) DESC")

names(counties) <- c("County", "Total Population", "White", "Black", "Hispanic")


n_blockgroups <- DBI::dbGetQuery(con, "SELECT COUNT(*) FROM race")
    
print(xtable(counties,
             caption="Populations of 49 Largest Counties (In Millions)",
             label="tab:counties"),
      digits=2,
      include.rownames=FALSE)
@ 

From the 2010-2015 ACS 5-Year Estimates, we pull the population of
Hispanics, Non-Hispanic Whites, and Non-Hispanic African Americans for
every block group in all the counties.\cite{bureau_american_????} For every
block group, we label the block group as Hispanic, White, or Black
based on which group is most common in the block group.  Using the
associated 2014 TIGER/Line Shapefiles, we calculate the adjacency
matrix for the
blockgroups\cite{tiger/line_????,rey_pysal:_2007}. There are \Sexpr{n_blockgroups} block groups in the data.

We'll use these data to estimate the parameters of the following score
function. 

\begin{align}
\operatorname{E}(\mathbf{y}) = \sum_{<i
  j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j) + \sum_i^N\epsilon_i(y_i)
\end{align}

Where 

\begin{align}
  \epsilon_{i,j} = &w_0\operatorname{WW}(x_{i}, x_{j}) +
  w_1\operatorname{WB}(x_{i}, x_{j}) + 
  w_2\operatorname{WH}(x_{i}, x_{j}) \\
  &+ w_3\operatorname{BB}(x_i, x_j) +
  w_4\operatorname{BH}(x_i, x_j) +
  w_5\operatorname{HH}(x_i, x_j)
\end{align} 

and $\operatorname{WW}$ indicates that block $i$ and $j$ are both
labeled as ``White''; $\operatorname{WB}$ indicates a ``White'' block
and ``Black'' block; $\operatorname{WH}$ indicates indicates a
``White'' block and ``Hispanic'' block; and so on.

In addition, 

\begin{align}
  \epsilon_{i} = w_6 + & w_7\operatorname{W_1}(y_{i}) +  
          w_{10}\operatorname{H_1}(y_{i}) + w_{11} +
          w_{12}\operatorname{B_1}(y_i) \\
          + &w_{12}\operatorname{W_2}(y_{i}) +
          w_{13}\operatorname{H_2}(y_{i}) +
          w_{14}\operatorname{B_2}(y_i)\\
          + &... \\
          + &w_{150}\operatorname{W_{49}}(y_{i}) +
          w_{151}\operatorname{H_{49}}(y_i) + w_{152}\operatorname{B_{49}}(y_i)
\end{align}

where $\operatorname{W_k}$, $\operatorname{B_k}$, $\operatorname{H_k}$
are indicator functions for whether a block is labeled as white,
black, or Hispanic. Since every county has a different racial mix, we
need a set indicator variables for each county. These weights affect
how we would label a block if we didn't know anything about it's neighbors.

We estimate all the parameters using the PyStruct
library.\cite{muller_pystruct_2014} In order to evaluate the
robustness of the parameters, we bootstrap a sampling distribution of
the parameters by sampling with replacement from the 49 counties,
estimating the parameters on the sample, recording the results, and
repeating 1000 times. Using the percentile bootstrap, we estimate the
confidence intervals for the parameters.

In Table~\ref{tab:parameters}, we have the parameters for racial
preferences.\footnote{If our formalism, which is typical of the
  literature, the objective is to find the parameters that give the
  observed data the smallest score. PyStruct attempts to find the
  parameters the largest score. This makes no real difference except
  it inverts the signs of the parameters. In order to maintain
  consistency, I invert the signs of PyStruct's parameters in this
  presentation} The values may seem very small, but this is due to the
constraint that $\sqrt{\sum_i^M w_i^2 = 1}$ for the 153
parameters. The racial preference parameters are between one to three
orders of magnitude greater than the city specific bias terms.

If a racial or ethnic preference parameter is negative, then blocks of
that type should tend to be neighbors, all else equal. If a preference
parameter is positive, then blocks of that type should tend to not be
neighbors. The signs of the parameter align with expectations. Each
racial and ethnic groups tend to live among their own group. White and
black blocks, and Hispanic and black groups tend to not to
neighbor. Interestingly, the parameter for white and Hispanic groups
is negative, suggesting that white and Hispanic groups should tend to
neighbor, all else being equal. Only the negative parameter for
``White-White'' and the postive parameter for ``Black-Hispanic'' have
95\% confidence intervals that do not cover 0.

\section*{Conclusion}
This relatively simple analysis demonstrate how we can estimate
parameters that agent-based models have to posit. Instead of moving
from micromotive to macrostructure, the support vector machine lets us
go from macrostructure to micromotives.

This machinery allows us to subject an important class of emergent
sociological problems to empirical tests. The nature and sources of
residential segregation are just one instance of a larger class of
problems where we are interested how network connections effect
discrete outcomes. Other examples can political party affiliations,
music genre preferences, or risk taking among groups of
acquaintances. Of course, when we move from a spatial network to a
social network we must take care to understand how an outcome could
structure the network in addition to how the network could influence
the behavior.

Within the field of urban sociology, the simple models that I have
used to introduced the method can be extended in a number of
ways. They can be used to comprehensively compare the potential
sources of racial segregation, by adding richer variables to the
particular block groups like the average rent and commute times. These
models can also be used to compare the racial preferences between
city, and provide a measure of racial segregation that is scale
free. These measures could be both input our outcome measures for
understanding the causes and effects of racial segregation. 


<<bootstrap>>=
library(RcppCNPy)

load <- function(x) {
    filename = paste0(x, '.npy')
    npyLoad(filename) * -1 #PyStruct's parameters have opposite signs of those typical in the literature
}

ww <- load('white-white')
bw <- load('black-white')
hw <- load("hispanic-white")
bb <- load("black-black")
hb <- load("hispanic-black")
hh <- load("hispanic-hispanic")

edges <- load("edge_param")
@ 


<<parameters, results='asis'>>=
confidence_end_points <- c(0.025, 0.975)

edge_parameters <- rbind(
    "White-White"=c(edges[1,1], quantile(ww, confidence_end_points)),
    "White-Black"=c(edges[2,1], quantile(bw, confidence_end_points)),
    "White-Hispanic"=c(edges[3,1], quantile(hw, confidence_end_points)),
    "Black-Black"=c(edges[2,2], quantile(bb, confidence_end_points)),
    "Black-Hispanic"=c(edges[3,2], quantile(hb, confidence_end_points)),
    "Hispanic-Hispanic"=c(edges[3,3], quantile(hh,confidence_end_points)))


interleave <- function(v1,v2) {
  ord1 <- 2*(1:length(v1))-1
  ord2 <- 2*(1:length(v2))
  c(v1,v2)[order(c(ord1,ord2))]
}

edge_parameters <- interleave(prettyNum(edge_parameters[,1], digits=2), 
                              paste0("(", 
                                     prettyNum(edge_parameters[,2], 
                                               digits=2), 
                                     ", ", 
                                     prettyNum(edge_parameters[,3], 
                                               digits=2), 
                                     ")"))
edge_parameters <- data.frame(names(edge_parameters), edge_parameters)
colnames(edge_parameters) <- c("", "Racial Compatibility")
edge_parameters[,2] <- as.character(edge_parameters[,2])
edge_parameters[1,2] <- paste0(edge_parameters[1,2], "*")
edge_parameters[9,2] <- paste0(edge_parameters[9,2], "*")
      
print(xtable(edge_parameters, align="llr", caption="Racial Compatibility Parameters\\\\ Numbers in Parenthesis are 95\\% Confidence Intervals (Percentile Bootstrap)\\\\
* $P\\leq.05$",
label="tab:parameters"), include.rownames=FALSE)

@ 

\newpage
\begingroup \parindent 0pt
\parskip 2ex
\def\enotesize{\normalsize}
\theendnotes
\endgroup

\newpage
\bibliographystyle{plainnat}
\bibliography{race-potts}



\end{document}

\footnote{We can also use these
  techniques on scoring functions that have a term for the relation
  between the properties of a particular block and the region
  assignment, $\epsilon_i(y_i)$. This is particularly useful when we
  already know something about what meaningful categories should exist.

  \begin{align}
    \operatorname{E}(\mathbf{y}) = \sum_i^N\epsilon_i(y_i) + \sum_{<i j>}^{\mathcal{N}}\epsilon_{i,j}(y_i,y_j)
  \end{align}

  For example, if we had a strong, ecological theory of urban space,
  we might want to identify regions like `the red light district,'
  `the zone of industry`, `the central business district.' We would
  want to pay a large penalty if we assigned a block that had lot of
  immigrants to `the central business district` instead of `the
  immigrant neighborhood.'
}

 
<<hist>>= 
par(mfrow=c(3,3))
hist(ww)
hist(bw)
hist(hw)
plot.new()
hist(bb)
hist(hb)
plot.new()
plot.new()
hist(hh)
par(mfrow=c(1,1))

@ 
